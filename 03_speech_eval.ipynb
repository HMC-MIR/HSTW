{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import librosa as lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalAllQueries(pairsFile, annotFile, pathsRoot, scoring_collar = 0.1, numThresholds = 1000):\n",
    "    insertionAttributions = []\n",
    "    deletionAttributions = []\n",
    "    replacementAttributions = []\n",
    "    \n",
    "    annotList = map(lambda x: x.split(), open(annotFile, 'r').readlines()) # I doubt this works but lets give it a go...\n",
    "    \n",
    "    with open(pairsFile, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            assert len(parts) == 2\n",
    "            \n",
    "            # Find correct annotations\n",
    "            queryId = os.path.basename(parts[0])\n",
    "            annot = findAnnot(annotList, queryId)\n",
    "            \n",
    "            # Find path\n",
    "            pathBasename = queryId + '__' + os.path.basename(parts[1])  # This will need to change based on file structure\n",
    "            pathFile = pathsRoot + '/' + pathBasename + '.pkl'\n",
    "            path = pkl.load(open(pathFile, 'rb'))\n",
    "\n",
    "            tamperType, theseAttributions = evalQuery(path, annot)\n",
    "            \n",
    "            # Add new costs and GT to\n",
    "            if tamperType == \"I\":\n",
    "                insertionAttributions += theseAttributions\n",
    "            elif tamperType == \"D\":\n",
    "                deletionAttributions += theseAttributions\n",
    "            else:\n",
    "                replacementAttributions += theseAttributions\n",
    "\n",
    "    # Get ROCs\n",
    "    insertionROC = calc_ROC(np.array(insertionAttributions), numTresholds)\n",
    "    deletionROC = calc_ROC(np.array(deletionAttributions), numTresholds)\n",
    "    replacementROC = calc_ROC(np.array(replacementAttributions), numTresholds)\n",
    "    \n",
    "    return [insertionROC, deletionROC, replacementROC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnnot(annotList, queryId):\n",
    "    for annot in annotList:\n",
    "        if annotList[0] == queryId:\n",
    "            return annot\n",
    "    \n",
    "    print(\"Error: Annotations not found\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalQuery(path, annot, scoring_collar):\n",
    "    \n",
    "    tamperType = annot[4]\n",
    "    attributions = []\n",
    "    \n",
    "    # set up boundaries list (in seconds, relative to modified query)\n",
    "    # In form matchingRegionStart, tamperStart, tamperEnd, matchingRegionEndEnd\n",
    "    # This will depend on tamperType\n",
    "    if tamperType == \"I\":\n",
    "        insertionStart = int(annot[5])\n",
    "        insertionLength = int(annot[8]) - int(annot[7])\n",
    "        insertionEnd = insertionStart + insertionLength\n",
    "        offset = 0\n",
    "        \n",
    "        # I use +/- inf here to signal that the matching region extends to the first and last frames, and no scoring\n",
    "        # collar is needed\n",
    "        boundaries = [-float('inf'), insertionStart, insertionEnd, float('inf')]\n",
    "\n",
    "    elif tamperType == \"D\": # For deletions, also flip query and reference\n",
    "        queryPath = path[:,0]\n",
    "        refPath = path[:,1]\n",
    "        path[:,0] = refPath\n",
    "        path[:1] = queryPath\n",
    "        \n",
    "        deletionStart = annot[5]\n",
    "        deletionEnd = annot[6]\n",
    "        offset = annot[2]\n",
    "        matchEnd = annot[3] - offset\n",
    "        \n",
    "        # Here, the matching region starts and ends at the boundaries of the query recording\n",
    "        boundaries = [0, deletionStart, deletionEnd, matchEnd]\n",
    "        \n",
    "    \n",
    "    else: # replacement\n",
    "        replacementStart = annot[5]\n",
    "        replacementEnd = annot[6]\n",
    "        offset = 0\n",
    "        \n",
    "        # Again, the matching region will extend all the way to the first and last frames\n",
    "        boundaries = [-float('inf'), replacementStart, replacementEnd, float('inf')]\n",
    "    \n",
    "    query_length = path[-1, 0] # NOTE: May need to change this based on path format\n",
    "    \n",
    "    GT = getAttributionsGT(query_length, offset, boundaries, scoring_collar, hop_sec)\n",
    "\n",
    "    # Impute cost scores\n",
    "    # Interpolate here to fill in the frames that the path jumps over\n",
    "    costs = np.interp(np.arange(offset / hop_sec, query_length), path[:,0], path[:,2])\n",
    "    for i in range(gt.shape[0]):\n",
    "        if gt[i] >=0:\n",
    "            attributions.append([gt[i], costs[i]])\n",
    "            \n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAttributionsGT(query_length, offset, boundaries, scoring_collar, hop_sec):\n",
    "    offsetFrames = offset / hop_sec\n",
    "    gt = np.zeros(query_length - offsetFrames)\n",
    "    \n",
    "    # Get the GT for each frame\n",
    "    # For now, just represent each frame with a single timestamp (at where the frame begins)\n",
    "    for frame in range(offset, query_length):\n",
    "        t_query = frame * hop_sec - offset\n",
    "        if withinCollar(t_query, boundaries, scoring_collar):\n",
    "            gt[frame] = -1\n",
    "        elif t_query < boundaries[0] or (t_query > boundaries[1] and t_query < boundaries[2]) or t_query > boundaries[3]:\n",
    "            gt[frame] = 1 # Non-matching region\n",
    "        else:\n",
    "            gt[frame] = 0\n",
    "            \n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinCollar(t_query, boundaries, scoring_collar):\n",
    "    for t_boundary in boundaries:\n",
    "        if np.abs(t_query - t_boundary) < scoring_collar:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ROC(attributions, numThresholds):\n",
    "    '''\n",
    "    Calculates ROC curve for attributions based on number of thresholds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = attributions[:,1]\n",
    "    gt = attributions[:,0]\n",
    "        \n",
    "        \n",
    "    # Get minimum and max for thresholds\n",
    "    thresholdMin = np.min(costs)\n",
    "    thresholdMax = np.max(costs)\n",
    "    \n",
    "    thresholds = np.linspace(thresholdMin, thresholdMax, numThresholds)\n",
    "    ROC = np.ones((numThresholds,3))*-1\n",
    "    \n",
    "    # For each threshold, calculate false positive and false negative (miss) rate\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        FPCountTot = 0\n",
    "        FNCountTot = 0\n",
    "        TrueNegCount = 0\n",
    "        TruePosCount = 0\n",
    "        \n",
    "        FPCount, FNCount = calcFPFN(costs, gt, threshold)\n",
    "\n",
    "        TrueNegCount += np.sum(gt == 0)\n",
    "        TruePosCount += np.sum(gt == 1)\n",
    "\n",
    "        FPCountTot += FPCount\n",
    "        FNCountTot += FNCount\n",
    "        \n",
    "        # Threshold, False Positive, False Negative\n",
    "        ROC[i,:] = [threshold, FPCountTot/TrueNegCount, FNCountTot/TruePosCount]\n",
    "    \n",
    "    return ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFPFN(costVec, gtAttribution, threshold):\n",
    "    '''\n",
    "    Calculate number of false positives and false negatives\n",
    "    '''\n",
    "    \n",
    "    # If cost is lower than threshold, then we consider sample in\n",
    "    # non tampered region (nontampered = 1, tampered = 0)\n",
    "    costHypVec = (costVec < threshold).astype(int)\n",
    "\n",
    "    diffVec = (costHypVec - gtAttribution)\n",
    "\n",
    "    FPCount = np.sum(np.maximum(diffVec,0))\n",
    "    FNCount = np.sum(np.minimum(diffVec,0))*-1\n",
    "\n",
    "    return FPCount, FNCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Run on data, Gen plots, Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIR",
   "language": "python",
   "name": "mir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
